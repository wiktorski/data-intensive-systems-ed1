#This file instructs how to setup a basic Hadoop environemnt 
#and run sample Hadoop and Hive jobs for
#Data Intensive Systems book and course

#I assume you have VirtualBox installed and downloaded 
#Hortonworks Sandbox, this tutorial was written using
#VirtualBox 4.3.30 and Sandbox 2.3 on Mac OS X 10.10
#You might need minor adjustments for Linux systems
#and major adjustments for Windows.

#Hortonworks Sandbox setup
File > Import Appliance, pick Sandbox ova file
Change name to "dis-sandbox", Reduce memory to 4096MB, Import
Start dis-sandbox

#log in to Sandbox VM from local machine
#edit /Users/your-user-name/.ssh/known_hosts if necessary
ssh root@127.0.0.1 -p 2222

#install pip and mrjob on Sandbox VM
yum -y install python-pip
pip install mrjob

#export HADOOP_PATH in .bash_profile
vi ~/.bash_profile
#add "export HADOOP_HOME=/usr/hdp/current/hadoop-client"

#copy Hadoop Streaming library to Hadoop home location
#original location of the library might change with Sandbox version
cp /usr/hdp/2.3.0.0-2557/hadoop-mapreduce/hadoop-streaming.jar $HADOOP_HOME
logout

#Accessing Hortonworks Sandbox and copying data
#copy materials from local machine to the VM
scp -P 2222 -r dis_materials root@127.0.0.1:~

#copy data from Sandbox VM to HDFS
ssh root@127.0.0.1 -p 2222
hadoop fs -mkdir /dis_materials
hadoop fs -put dis_materials/*.txt dis_materials/*.csv /dis_materials

#Analyzing unstructured data with Hadoop Streaming
cd dis_materials
hadoop jar $HADOOP_HOME/hadoop-streaming.jar \
    -mapper email_count_mapper.py \
    -reducer email_count_reducer.py \
    -input /dis_materials/hadoop_1m.txt \
    -output /dis_materials/output1 \
    -file email_count_mapper.py \
    -file email_count_reducer.py

hadoop fs -text /dis_materials/output1/part* | less

#Analysing unstructured data with MRJob
python count_sum.py -r hadoop hdfs:///dis_materials/hadoop_1m.txt --output-dir hdfs:///dis_materials/output2 --no-output

hadoop fs -text /dis_materials/output2/part* | less

#Testing code without any Hadoop installation
cat hadoop_1m.txt | ./email_count_mapper.py | sort -k1,1 | ./email_count_reducer.py
python count_sum.py -r inline hadoop_1m.txt

#Analyzing structured data with Hive
#start hive from Sandbox or cluster
hive

#create table and import data
CREATE EXTERNAL TABLE hadoopmail (list STRING, date1 STRING, date2 STRING, email STRING, topic STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

LOAD DATA INPATH '/dis_materials/hadoop.csv' OVERWRITE INTO TABLE hadoopmail;

#run a simple data query
SELECT substr(email, locate('@', email)+1), count(substr(email,locate('@', email)+1)) FROM hadoopmail GROUP BY substr(email,locate('@', email)+1);